{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: [Daniel Puente Viejo](https://www.linkedin.com/in/danielpuenteviejo/)*\n",
    "\n",
    "## **Create an Agent System with completly free LLM and Autogen for Course Recommendation**\n",
    "\n",
    "\n",
    "In this tutorial, we will show how to use the LangGraph and Mistral to create a simple CV evaluator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1' style=\"color: skyblue;\">**1 - Libraries**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import autogen\n",
    "from autogen import UserProxyAgent, register_function, AssistantAgent\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termination_message(msg):\n",
    "    content = msg.get(\"content\", \"\").lower()\n",
    "    return \"terminate\" in content\n",
    "\n",
    "# File path for the CV\n",
    "cv_file_path = \"data/cv.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It works FINE.\n",
    "llama31_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"llama-3.1-8b-instant\",\n",
    "            \"api_key\": os.environ.get(\"GROQ_API_KEY\"),\n",
    "            \"api_type\": \"groq\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "gemma2_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"gemma2-9b-it\",\n",
    "            \"api_key\": os.environ.get(\"GROQ_API_KEY\"),\n",
    "            \"api_type\": \"groq\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user proxy agent that executes tool calls.\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=termination_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"use_docker\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Agent 1: CV Analyser --\n",
    "cv_analyser = AssistantAgent(\n",
    "    name=\"CV Analyser\",\n",
    "    description=\"It analyzes the input it giving the most relevant information of the document.\",\n",
    "    system_message=\"\"\"You must return an analysis of the input file.\n",
    "    Other details for your answer:\n",
    "    - Do not responde any piece of code.\n",
    "    - Do not recommend courses just analyse the input file.\"\"\",\n",
    "    llm_config=llama31_config,\n",
    ")\n",
    "\n",
    "## -- Agent 2: Skills Extractor --\n",
    "skills_extractor = AssistantAgent(\n",
    "    name=\"Skills Extractor\",\n",
    "    description=\"Returns a list of the candidates skills\",\n",
    "    system_message=\"\"\"Your task is to extract the main skills from the input received.\n",
    "    You must return the skills as a comma-separated list. For example: 'Python, Java, SQL'. Do not add more information, just the list of skills in the specified format.\n",
    "    Other details for your answer:\n",
    "    - Do not responde any piece of code.\n",
    "    - Do not recommend courses just return the skills as a comma-separated list of the input file.\"\"\",\n",
    "    llm_config=llama31_config,\n",
    ")\n",
    "\n",
    "## -- Agent 3: Courses Recommender --\n",
    "courses_recommender = AssistantAgent(\n",
    "    name=\"Courses recommender\",\n",
    "    description=\"The goal is to recommend courses. Read the courses from `data/courses.txt` and return the courses that match the skills of the input file.\",\n",
    "    system_message=\"\"\"The goal is to recommend courses, for that follow these steps:\n",
    "    1) Read the list of courses from `data/courses.txt` (use `read_txt_courses` tool).\n",
    "    2) Combine the list of courses with the profile of the candidate.\n",
    "    3) Return the courses that match the skills of the candidate.\n",
    "    Other details for your answer: Do not responde any piece of code.\"\"\",\n",
    "    llm_config=llama31_config,\n",
    ")\n",
    "\n",
    "# --- Agent 4: Resolution Checker Agent ---\n",
    "resolution_checker = AssistantAgent(\n",
    "    name=\"Resolution Checker Agent\",\n",
    "    description=\"Checks if the user's query has been resolved. If resolved, respond with 'TERMINATE', otherwise respond with 'CONTINUE'. Do not give more information just the word 'CONTINUE' or 'TERMINATE'.\",\n",
    "    system_message=\"\"\"You should respond with 'TERMINATE' if the query has been resolved. Otherwise, respond with 'CONTINUE'. Do not add more information, just the word 'CONTINUE'.\n",
    "    Try to be resolute, do not reply 'TERMINATE' if the initial query has not been resolved.\n",
    "    **IMPORTANT**: Only respond with 'TERMINATE' if the query has been resolved, otherwise respond with 'CONTINUE'. If the answer is 'TERMINATE', you must answer 'TERMINATE' + generate a complete answer to the user's query considering all the information given by all the agents to generate the final answer.\"\"\",\n",
    "    llm_config=gemma2_config,\n",
    "    is_termination_msg=termination_message,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool: Read CV from File\n",
    "def read_txt(path: str) -> str:\n",
    "    path = path.split(\"/\")[-1]\n",
    "    path = f\"data/{path}\"\n",
    "    with open(path, \"r\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the read_txt function with the assistant agent.\n",
    "register_function(\n",
    "    read_txt,\n",
    "    caller=cv_analyser,\n",
    "    executor=user_proxy,\n",
    "    name=\"read_txt_cv_analyse\",\n",
    "    description=\"Read the profile of the user\",\n",
    ")\n",
    "\n",
    "register_function(\n",
    "    read_txt,\n",
    "    caller=courses_recommender,\n",
    "    executor=user_proxy,\n",
    "    name=\"read_txt_courses\",\n",
    "    description=\"Read available courses and/or the profile of the user\",\n",
    ")\n",
    "\n",
    "register_function(\n",
    "    read_txt,\n",
    "    caller=skills_extractor,\n",
    "    executor=user_proxy,\n",
    "    name=\"read_txt_cv_skills\",\n",
    "    description=\"Read the profile of the user\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GroupChatManager to oversee the conversation\n",
    "def custom_speaker_selection(last_speaker, groupchat):\n",
    "    \"\"\"\n",
    "    Function to change the agent selection logic and customize the speaker selection.\n",
    "\n",
    "    Args:\n",
    "    last_speaker (Agent): The last agent that spoke.\n",
    "    groupchat (GroupChat): The group chat object.\n",
    "\n",
    "    Returns:\n",
    "    Agent: The agent that will speak next.\n",
    "    \"\"\"\n",
    "    agents_to_check = [\"cv analyser\", \"skills extractor\", \"courses recommender\"]\n",
    "\n",
    "    # Extract the last and penultimate messages\n",
    "    last_message = groupchat.messages[-1]\n",
    "    penultimate_message = groupchat.messages[-2] if len(groupchat.messages) > 1 else None\n",
    "\n",
    "    # Extract the names of the last and penultimate speakers\n",
    "    last_speaker_name = last_speaker.name.lower()\n",
    "    penultimate_speaker_name = penultimate_message['name'] if penultimate_message else None\n",
    "\n",
    "    ### If the agent has made a tool call, call the 'User'\n",
    "    if last_message.get(\"tool_calls\", \"\"):\n",
    "        return next(agent for agent in groupchat.agents if agent.name == \"User\")\n",
    "\n",
    "    ### If the last speaker was the 'User' and the penultimate speaker was not the user or the Resolution Checker Agent, call the penultimate speaker\n",
    "    ### Example: Skills Extractor makes use of a tool, so it call the 'User' agent. The 'User' agent then calls the 'Skills Extractor' to continue with the task it was doing.\n",
    "    if last_speaker_name == \"user\" and penultimate_speaker_name and penultimate_speaker_name.lower() not in [\"user\", \"resolution checker agent\"]:\n",
    "        return next(agent for agent in groupchat.agents if agent.name == penultimate_speaker_name)\n",
    "\n",
    "    ### The an agent has given an answer, check if the quey is complete. For that call the 'Resolution Checker Agent'\n",
    "    if last_speaker_name in agents_to_check:\n",
    "        return next(agent for agent in groupchat.agents if agent.name == \"Resolution Checker Agent\")\n",
    "\n",
    "    ### If no condition is met, leave the automatic configuration so that the next agent selection is based on an LLM.\n",
    "    return 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[\n",
    "        user_proxy,\n",
    "        cv_analyser,\n",
    "        skills_extractor,\n",
    "        courses_recommender,\n",
    "        resolution_checker,\n",
    "    ],\n",
    "    messages=[],\n",
    "    allow_repeat_speaker=True,\n",
    "    speaker_selection_method=custom_speaker_selection,\n",
    "    role_for_select_speaker_messages=\"system\"\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    code_execution_config={\"use_docker\": False},\n",
    "    llm_config=gemma2_config,\n",
    "    is_termination_msg=termination_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to chat_manager):\n",
      "\n",
      "Extract skills in comma-separated format based on the following profile: data/cv.txt.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "AUTO 4)\n",
      "\u001b[32m\n",
      "Next speaker: Skills Extractor\n",
      "\u001b[0m\n",
      "\u001b[33mSkills Extractor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_6v4h): read_txt_cv_skills *****\u001b[0m\n",
      "Arguments: \n",
      "{\"path\": \"data/cv.txt\"}\n",
      "\u001b[32m***************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Tool 1)\n",
      "\u001b[32m\n",
      "Next speaker: User\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION read_txt_cv_skills...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_6v4h) *****\u001b[0m\n",
      "Daniel Puente Viejo\n",
      "+34 638 097 547\n",
      "daniel.puenteviejo@gmail.com\n",
      "https://github.com/DanielPuentee | https://www.linkedin.com/in/danielpuenteviejo | https://medium.com/@daniel.puenteviejo | https://huggingface.co/D0k-tor\n",
      "\n",
      "---\n",
      "\n",
      "PROFESSIONAL EXPERIENCE\n",
      "\n",
      "NTT Data\n",
      "➢ Clients: Cepsa, Repsol, Unsere Grüne Glasfaser, RED.es, BBVA, NTT Data (Intern client)\n",
      "\n",
      "External Senior AI Engineer at Repsol; Bilbao, Spain\n",
      "*10/2024 - Present*\n",
      "- Led the design of Repsol's cross AI Multi-Agent platform and LLMOps platform, including LLM API Management, to support scalable generative AI applications (Azure-oriented solutions).\n",
      "- Designed and managed the technical design of multiple Gen AI MVPs, driving innovation in all business areas.\n",
      "\n",
      "Generative AI II Engineer; Bilbao, Spain\n",
      "*05/2024 - 10/2024*\n",
      "- Developer of a Generative AI platform on AWS, valued at €25M, for the Spanish Ministry of Digital Transformation (AWS Architecture, RAG, clause extraction, plagiarism detection...).\n",
      "- Reduced amendment generation time by 95% by developing LLM and OCR techniques implemented in AWS.\n",
      "\n",
      "Generative AI I Engineer; Bilbao, Spain\n",
      "*09/2023 - 05/2024*\n",
      "- Reduced time for legal information search, generation, and question answering by 90% through the creation of a RAG system using Azure Cognitive Services (Repsol's first generative AI use case).\n",
      "- Led the improvement of executive information search time through a RAG system using Dolffia and OpenAI Whisper.\n",
      "\n",
      "Junior Artificial Intelligence Engineer (Intern); Bilbao, Spain\n",
      "*09/2022 - 07/2023*\n",
      "- Improved explainability and precision (~80%) of credit fraud detection by applying Azure ML and GNN.\n",
      "- Encouraged learning by creating an AI initiative with over 20 current members.\n",
      "  - Developed a contact detection algorithm for the NFL using Azure ML.\n",
      "  - Applied diffusion algorithms for automatic image description.\n",
      "\n",
      "Navarra University\n",
      "Data Science Professor; Pamplona, Spain\n",
      "*09/2023 - Present*\n",
      "- Develop Generative AI course for BBVA Data Scientists (RAG, LLM fine-tuning, and RAG evaluation).\n",
      "- Professor of Data Science in the Business + Data Analytics and Economics + Data Analytics double degree programmes.\n",
      "\n",
      "Mondragon University\n",
      "➢ Clients: BBVA, Ormazabal, Sener, Laboral Kutxa, Mondragon Corporation, Lookiero…\n",
      "\n",
      "Student Data Scientist; Bilbao, Spain\n",
      "*09/2019 - 12/2022*\n",
      "- Achieved 0% risk of lithium-ion battery explosion by developing LSTM + GAN models.\n",
      "- Reduced cost by 15% and improved engine design efficiency by 2% using evolutionary algorithms and RL.\n",
      "- Developed an automatic fashion look recommendation system using graph neural networks (GNN).\n",
      "- 42%-time reduction of Monte Carlo simulations using Recurrent Neural Networks.\n",
      "\n",
      "---\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Master of Science in Artificial Intelligence - UNIR; Online\n",
      "*10/2023 - 08/2024*\n",
      "Final Mark: 9,5/10 · A+ | MSc Thesis: Implementation and Evaluation of a RAG System for the Resolution of Legal Queries.\n",
      "\n",
      "Bachelor of Science in AI & Data Analytics - Mondragon University (1º Promotion); Bilbao, Spain\n",
      "*09/2019 - 07/2023*\n",
      "Final Mark: 9,0/10 · A | BSc Thesis: Simulation of a graph-based fraud detection algorithm deployed in a production Azure.\n",
      "\n",
      "Bachelor of Science in Mathematical Sciences - Essex University; Colchester, England\n",
      "*04/2022 - 07/2022*\n",
      "Final Mark: 8,6/10 · B+\n",
      "\n",
      "---\n",
      "\n",
      "LANGUAGES\n",
      "- Spanish (Native)\n",
      "- English (Cambridge Certificate Advanced English, C1)\n",
      "- Basque (B2)\n",
      "\n",
      "---\n",
      "\n",
      "SKILLS\n",
      "- Programming: Python • R • SQL\n",
      "- AI Frameworks: Langchain • CrewAI • Ollama • Hugging Face • Pytorch\n",
      "- Cloud: Azure • AWS • Azure AI Services • Bedrock\n",
      "- Other utilities: Finetuning • RAG • CUDA • Git • Databricks • GNN • QLoRA\n",
      "\n",
      "---\n",
      "\n",
      "AWARDS & HONOURS\n",
      "Certifications and Personal Projects\n",
      "- Completed over 30 courses in AI, including Microsoft Certified: DP-100, AI-102, DP-900 & AI-900.\n",
      "- Selected among the 5 best Bachelor Thesis out of 300 candidates in the Mondragon Corporation Awards.\n",
      "- Selected as one of around 250 participants in the Bizkaia Talent program “attracting young talents”.\n",
      "- Scholarship for academic excellence (2019).\n",
      "\n",
      "---\n",
      "\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Penultimate 2) Skills Extractor\n",
      "\u001b[32m\n",
      "Next speaker: Skills Extractor\n",
      "\u001b[0m\n",
      "\u001b[33mSkills Extractor\u001b[0m (to chat_manager):\n",
      "\n",
      "Python, R, SQL, Langchain, CrewAI, Ollama, Hugging Face, Pytorch, Azure, AWS, Azure AI Services, Bedrock, Finetuning, RAG, CUDA, Git, Databricks, GNN, QLoRA\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Resolution Checker Agent 3)\n",
      "\u001b[32m\n",
      "Next speaker: Resolution Checker Agent\n",
      "\u001b[0m\n",
      "\u001b[33mResolution Checker Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "CONTINUE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "AUTO 4)\n",
      "\u001b[32m\n",
      "Next speaker: Courses recommender\n",
      "\u001b[0m\n",
      "\u001b[33mCourses recommender\u001b[0m (to chat_manager):\n",
      "\n",
      "<function=split_string>\"Python, R, SQL, Langchain, CrewAI, Ollama, Hugging Face, Pytorch, Azure, AWS, Azure AI Services, Bedrock, Finetuning, RAG, CUDA, Git, Databricks, GNN, QLoRA\"</split_string>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Resolution Checker Agent 3)\n",
      "\u001b[32m\n",
      "Next speaker: Resolution Checker Agent\n",
      "\u001b[0m\n",
      "\u001b[33mResolution Checker Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "CONTINUE \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "AUTO 4)\n",
      "\u001b[32m\n",
      "Next speaker: User\n",
      "\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "AUTO 4)\n",
      "\u001b[32m\n",
      "Next speaker: CV Analyser\n",
      "\u001b[0m\n",
      "\u001b[33mCV Analyser\u001b[0m (to chat_manager):\n",
      "\n",
      "\"Python\", \"R\", \"SQL\", \"Langchain\", \"CrewAI\", \"Ollama\", \"Hugging Face\", \"Pytorch\", \"Azure\", \"AWS\", \"Azure AI Services\", \"Bedrock\", \"Finetuning\", \"RAG\", \"CUDA\", \"Git\", \"Databricks\", \"GNN\", \"QLoRA\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Resolution Checker Agent 3)\n",
      "\u001b[32m\n",
      "Next speaker: Resolution Checker Agent\n",
      "\u001b[0m\n",
      "\u001b[33mResolution Checker Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE  \n",
      "\n",
      "Skills: Python, R, SQL, Langchain, CrewAI, Ollama, Hugging Face, Pytorch, Azure, AWS, Azure AI Services, Bedrock, Finetuning, RAG, CUDA, Git, Databricks, GNN, QLoRA   \n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the message to be sent to the manager\n",
    "message = f\"Recommend courses for the profile {cv_file_path}\"\n",
    "message = f\"Analyse the following profile: {cv_file_path}.\"\n",
    "message = f\"Extract skills in comma-separated format based on the following profile: {cv_file_path}.\"\n",
    "message = f\"Recommends courses for the profile {cv_file_path}. Return also the profile skills in a list.\"\n",
    "\n",
    "chat_result = user_proxy.initiate_chat(\n",
    "    recipient=manager,\n",
    "    message=message,\n",
    "    max_turns=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User', 'Skills Extractor', 'User', 'Skills Extractor', 'Resolution Checker Agent', 'Courses recommender', 'Resolution Checker Agent', 'User', 'CV Analyser', 'Resolution Checker Agent']\n"
     ]
    }
   ],
   "source": [
    "agent_order = [x['name'] for x in chat_result.chat_history]\n",
    "print(agent_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERMINATE  \n",
      "\n",
      "Skills: Python, R, SQL, Langchain, CrewAI, Ollama, Hugging Face, Pytorch, Azure, AWS, Azure AI Services, Bedrock, Finetuning, RAG, CUDA, Git, Databricks, GNN, QLoRA   \n"
     ]
    }
   ],
   "source": [
    "print(chat_result.chat_history[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCrew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
